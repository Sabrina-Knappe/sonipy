
@article{Patton:2019,
	title = {Sonification of {Transient} {Lightcurves}: {Supernovae} {Case} {Studies}},
	volume = {233},
	shorttitle = {Sonification of {Transient} {Lightcurves}},
	url = {http://adsabs.harvard.edu/abs/2019AAS...23314712P},
	abstract = {While detailed analyses and spectroscopic follow-up of transient events requires professional experience and access to observing facilities, lightcurve classification - distinguishing between several subtypes of supernovae, or identifying different classes of variable stars - can be accomplished by citizen scientists through template matching and comparisons. In a first for citizen science astronomy, we're moving beyond visual inspection analysis to the sonification of lightcurve data for classification by public volunteers on Zooniverse. Our approach to sonification maps each magnitude data point to a corresponding audible frequency, producing audio files that depict magnitude variations as perceptually uniform changes in pitch through time. Using this method, auditory classification can be applied to lightcurves that vary by up to ∼7 magnitudes, encompassing most known lightcurves for variable
stars, supernovae, and other transient events such as LBV eruptions, while allowing the listener to perceive a minimum difference of
△m=0.02 mags. We present the successfully sonified audio light
curves of a collection of transient phenomena, including a large and diverse sample of supernova lightcurves and test cases for RR Lyrae, LBVs, and short- and long-period eclipsing binaries. We demonstrate that linear and plateau supernova light curves can be audibly differentiated. This approach offers a new method for lightcurve classification and, for the first time, opens citizen science astronomy to participants who are visually impaired. TransientZoo will be launched in the next few years on Zooniverse and will ultimately be optimized and scaled for use with LSST.},
	urldate = {2020-07-02},
	author = {Patton, Locke and Levesque, Emily},
	month = jan,
	year = {2019},
	note = {Conference Name: American Astronomical Society Meeting Abstracts \#233},
	pages = {147.12}
}

@book{Kollmeier:2008,
	title = {Perception of {Speech} and {Sound}},
	isbn = {978-3-540-49125-5},
	abstract = {The transformation of acoustical signals into auditory sensations can be characterized by psychophysical quantities such as loudness, tonality, or perceived pitch. The resolution limits of the auditory system produce spectral and temporal masking phenomena and impose constraints on the perception of amplitude modulations. Binaural hearing (i.e., utilizing the acoustical difference across both ears) employs interaural time and intensity differences to produce localization and binaural unmasking phenomena such as the binaural intelligibility level difference, i.e., the speech reception threshold difference between listening to speech in noise monaurally versus listening with both ears. The acoustical information available to the listener for perceiving speech even under adverse conditions can be characterized using the articulation index, the speech transmission index, and the speech intelligibility index. They can objectively predict speech reception thresholds as a function of spectral content, signal-to-noise ratio, and preservation of amplitude modulations in the speech waveform that enter the listenerÊŒs ear. The articulatory or phonetic information available to and received by the listener can be characterized by speech feature sets. Transinformation analysis allows one to detect the relative transmission error connected with each of these speech features. The comparison across man and machine in speech recognition allows one to test hypotheses and models of human speech perception. Conversely, automatic speech recognition may be improved by introducing human signal-processing principles into machine processing algorithms.},
	booktitle = {Springer {Handbook} of {Speech} {Processing}},
	author = {Kollmeier, Birger and Brand, Thomas and Meyer, Bernd},
	month = jan,
	year = {2008},
	doi = {10.1007/978-3-540-49127-9_4},
	note = {Journal Abbreviation: Springer Handbook of Speech Processing},
	pages = {61--82}
}

@book{Downey:2018,
	title = {{ThinkDSP}: {Think} {DSP}: {Digital} {Signal} {Processing} in {Python}, by {Allen} {B}. {Downey}},
	shorttitle = {{ThinkDSP}},
	url = {https://github.com/AllenDowney/ThinkDSP},
	urldate = {2018-01-22},
	author = {Downey, Allen},
	month = jan,
	year = {2018},
	note = {original-date: 2013-08-13T18:37:02Z},
	file = {Snapshot:/Users/lockepatton/Zotero/storage/S3NCZJHK/ThinkDSP.html:text/html}
}
